{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commented MNIST example\n",
    "\n",
    "This notebook presents a full commented implementation of the jacobian optimization method introduced in [1][Relative gradient optimiztion of the Jacobian term in unsupervised deep learning](https://arxiv.org/pdf/2006.15090.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The method has been implemented using Python and [JAX](https://github.com/google/jax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import time\n",
    "\n",
    "from jax                 import random, vmap, jit, grad\n",
    "from jax.nn.initializers import orthogonal\n",
    "from jax.scipy.stats     import norm\n",
    "from jax.experimental    import optimizers as opt\n",
    "\n",
    "from utils               import shuffle_perm, batchify\n",
    "from itertools           import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers   = 2\n",
    "seed       = 42\n",
    "n_epochs   = 25\n",
    "lr         = 1e-4\n",
    "batch_size = 10\n",
    "\n",
    "# JAX random generator key\n",
    "key = random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training dataset\n",
    "\n",
    "We load the MNIST training and validation datasets as simple matrices of size NxD where N will usually be the `batch_size` and D = 784 (MNIST flattened dimensionality). As the first dimension is the batch dimension, we assume the samples to be arranged by rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataloaders import load_dataset\n",
    "x, val_data, _, _ = load_dataset(\"MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "We implement the `smooth_leaky_relu` activation function\n",
    "$\n",
    "\\begin{equation}\n",
    "    \\operatorname{s_L}(x) = \\alpha x + (1 - \\alpha) \\log (1 + e^x)\n",
    "    \\label{eq:sl_relu}\n",
    "\\end{equation}\n",
    "$ and the `Dense` fully-connected layer we use. The `dummy` argument to the `Dense` layer represents the `Accumulator` layer described in Appendix D of [[1]](https://arxiv.org/pdf/2006.15090.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Smoothed version of the leaky relu activation function\n",
    "    Inspiration:\n",
    "    https://stats.stackexchange.com/questions/329776/approximating-leaky-relu-with-a-differentiable-function\n",
    "    \"\"\"\n",
    "    return alpha*x + (1 - alpha)*(jnp.logaddexp(x, 0)) # - jnp.log(2.0))\n",
    "\n",
    "nonlinearity = smooth_leaky_relu\n",
    "\n",
    "def Dense(params, x, dummy=0):\n",
    "    W, b = params\n",
    "    return jnp.dot(x, W) + dummy + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the functions to initialize the parameters and build the model. In the end, the model will be specified by its `params` and 3 `g` functions; all the 3 functions perform a forward pass through the model, but with some differences:\n",
    "\n",
    "* `g_dummy` takes 3 arguments: `params`, `x`, `dummy`. The `dummy` argument is needed to be able to accumulate the $\\delta$ gradients when needed (Appendix D of [[1]](https://arxiv.org/pdf/2006.15090.pdf)).\n",
    "* `g_layerwise` returns the output `z` (the latent variables) and the activations `ys` for each layer (no non-linearities applied).\n",
    "* `g`: returns `z` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy_params(params, x):\n",
    "    return [jnp.zeros(x.shape) for p in params]\n",
    "\n",
    "def get_model(n_features, n_layers,\n",
    "              nonlinearity=smooth_leaky_relu,\n",
    "              key=random.PRNGKey(0),\n",
    "              W_init=orthogonal()):\n",
    "\n",
    "    # Forward pass through the network.\n",
    "    # Returns the latent configurations `z`\n",
    "    # and the activations at intermediate layers `ys`.\n",
    "    def forward(params, dummies, x):\n",
    "\n",
    "        z = x\n",
    "\n",
    "        def step(Wb, dummy):\n",
    "            nonlocal z\n",
    "            y = Dense(Wb, z, dummy=dummy)\n",
    "            z = nonlinearity(y)\n",
    "            return y\n",
    "\n",
    "        ys = [step(Wb, dummy) for (Wb, dummy) in zip(params[:-1], dummies[:-1])]\n",
    "\n",
    "        # last layer (no nonlinearity)\n",
    "        z = Dense(params[-1], z, dummies[-1])\n",
    "\n",
    "        return z, ys\n",
    "\n",
    "    g_dummy = forward\n",
    "\n",
    "    def g_layerwise(params, x):\n",
    "        dummies = get_dummy_params(params, x)\n",
    "        return g_dummy(params, dummies, x)\n",
    "\n",
    "    g = lambda params, x: g_layerwise(params, x)[0]\n",
    "\n",
    "    # parameters init\n",
    "    def init_Wb(key, n_features):\n",
    "        return W_init(key, (n_features, n_features)), jnp.zeros(n_features)\n",
    "\n",
    "    params = [init_Wb(k, n_features) for k in random.split(key, n_layers)]\n",
    "\n",
    "    return params, g_dummy, g_layerwise, g\n",
    "\n",
    "params, g_dummy, g_layerwise, g = get_model(x.shape[-1], n_layers,\n",
    "                                    nonlinearity=smooth_leaky_relu,\n",
    "                                    key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The objective function to optimize is the loglikelihood of our model $\\boldsymbol{L}(\\boldsymbol{x}) = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ that we rewrite as $\\boldsymbol{L}(\\boldsymbol{x}) = \\boldsymbol{L}_p(\\boldsymbol{x}) + \\boldsymbol{L}_J(\\boldsymbol{x})$ with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{L}_p (\\boldsymbol{x}) = \\sum_i \\log p_i(\\boldsymbol{g}^i_{\\boldsymbol{\\theta}}(\\boldsymbol{x})) ; \\enspace \\ \\boldsymbol{L}_J (\\boldsymbol{x}) = \\log \\left|\\det \\boldsymbol{J}\\boldsymbol{g}_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\right|\\,, \n",
    "$$\n",
    "\n",
    "We further separate $\\boldsymbol{L}_J$ in a \"layerwise\" component $\\boldsymbol{L}_J^1$ depending on the intermediate layers activations $\\boldsymbol{y}_k$ and a component $\\boldsymbol{L}_J^2$ depending on the parameters only\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\boldsymbol{L}_J (\\boldsymbol{z}_{k-1}) &= \\sum_{i=1}^D\\log \\left| \\sigma'(y_{k}^{i}) \\right| + \\log \\left| \\det \\boldsymbol{W}_k \\right| \\\\\n",
    "    &=: \\boldsymbol{L}_J^{1}(\\boldsymbol{y}_{k}) + \\boldsymbol{L}_J^{2}(\\boldsymbol{z}_{k-1})\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The gradient of the $\\boldsymbol{L}_J^2$ term is calculated explicitly and can be computed efficiently by making use of the relative gradient trick (see section 4 of [[1]](https://arxiv.org/pdf/2006.15090.pdf)). Through automatic differentiation we optimize the latent variables distribution of choice $\\boldsymbol{L}_p$ (a standard Gaussian in this case) and the `loss_layerwise` term $\\boldsymbol{L}_J^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "@vmap\n",
    "def log_pdf_normal(s):\n",
    "    \"\"\" Log-pdf for a Gaussian distribution w. mean 0 std 1\"\"\"\n",
    "    return jnp.sum(norm.logpdf(s))\n",
    "\n",
    "def inner_layerwise(act_prime, y):\n",
    "    return jnp.log(vmap(vmap(act_prime))(y))\n",
    "\n",
    "def loss_layerwise(nonlinearity, ys):\n",
    "    act_prime = grad(nonlinearity)\n",
    "    batched = vmap(inner_layerwise, in_axes=(None, 0))\n",
    "\n",
    "    # summing individual layers contributions\n",
    "    # Note: this works fine even with `len(ys)` == 2\n",
    "    full_pass = jnp.sum(batched(act_prime, jnp.stack(ys)), axis=0)\n",
    "\n",
    "    # summing over dimension\n",
    "    return jnp.sum(full_pass, axis=1)\n",
    "\n",
    "# Here the `dummies` argument is needed to be able to compute the `delta` terms (Appendix D in [1])\n",
    "# through the JAX `grad` function.\n",
    "def loss(params, dummies, x):\n",
    "\n",
    "    z, ys = g_dummy(params, dummies, x)\n",
    "\n",
    "    lpdf = log_pdf_normal(z)\n",
    "    lwise = loss_layerwise(nonlinearity, ys)\n",
    "\n",
    "    l = - sum(jnp.mean(li) for li in [lpdf, lwise])\n",
    "\n",
    "    return l, (z, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative gradient\n",
    "\n",
    "Here we compute the gradient and apply the relative trick taking care of avoiding matrix-matrix multiplications as described in Section 4 of [[1]](https://arxiv.org/pdf/2006.15090.pdf).\n",
    "\n",
    "The core of the computation happens in the `apply_trick` function, where the updates are computed following Appendix F of [[1]](https://arxiv.org/pdf/2006.15090.pdf) to include biases (note that we transposed the update expressions as we are working with row vectors).\n",
    "\n",
    "The `add_det_grad` function completes the gradient updates by including the gradients of the $\\boldsymbol{L}_J^2$ term of the loglikelihood, that we compute explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_gradient(gradient):\n",
    "\n",
    "    def apply_trick(Wb, z, bp_terms):\n",
    "        W, b = Wb\n",
    "        db, delta = bp_terms\n",
    "\n",
    "        dW = W @ (W.T @ z.T) @ delta + W @ b.T @ db.T\n",
    "        db = db * (1 + b @ b.T) + b @ W.T @ z.T @ delta\n",
    "\n",
    "        return (dW, db)\n",
    "\n",
    "    def relative_gradient(params, x):\n",
    "\n",
    "        dummies = get_dummy_params(params, x)\n",
    "        ((grads, deltas), (z, ys)) = gradient(params, dummies, x)\n",
    "\n",
    "        dbs = (db for (dW, db) in grads)\n",
    "        bp_terms = zip(dbs, deltas) # backprop terms\n",
    "        zs = chain([x], (nonlinearity(y) for y in ys))\n",
    "\n",
    "        return [apply_trick(*args) for args in zip(params, zs, bp_terms)]\n",
    "\n",
    "    return relative_gradient\n",
    "\n",
    "def add_det_grad(gradient):\n",
    "\n",
    "    def det_grad(params, x):\n",
    "        grad_params = gradient(params, x)\n",
    "        return [(dW - W, db) for ((dW, db), (W, b)) in zip(grad_params, params)]\n",
    "\n",
    "    return det_grad\n",
    "\n",
    "gradient = grad(loss, argnums = (0, 1), has_aux = True)\n",
    "gradient = add_det_grad(get_relative_gradient(gradient))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "The `train` function accepts a list of `loggers` to log scalar values. Each logger must be a function taking arguments `params` and `epoch` and must return a tuple: the first element is a string indicating the name of the logged quantity, the second element is the value to log. We define in this way a function `log_loss` to log the loglikelihood on the validation set during training, and a `log_time` function to log the execution time of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the term L^2_J of the loglikelihood\n",
    "def log_abs_det(params):\n",
    "    Ws = [W for (W, b) in params]\n",
    "    return jnp.sum(jnp.linalg.slogdet(Ws)[1])\n",
    "\n",
    "# Note that here we want to log the full loglikelihood;\n",
    "# during training we directly optimize only the term `l1 + l2`\n",
    "# and we include the gradient of the term `l3` explicitly\n",
    "# (i.e. the `loss` function we derive include only `l1 + l2`\n",
    "# and the `l3` term is introduced with `add_det_grad`)\n",
    "#@jit\n",
    "def full_loss(params, x):\n",
    "    z, ys = g_layerwise(params, x)\n",
    "\n",
    "    l1 = jnp.mean(log_pdf_normal(z))\n",
    "    l2 = jnp.mean(loss_layerwise(nonlinearity, ys))\n",
    "    l3 = log_abs_det(params)\n",
    "\n",
    "    print(\"pwise loss:\", l1, l2, l3)\n",
    "\n",
    "    return l1 + l2 + l3\n",
    "\n",
    "def get_loss_logger():\n",
    "    \n",
    "    loss_vs_time = []\n",
    "    \n",
    "    def log_loss(params, epoch):\n",
    "        val_loss = full_loss(params, val_data)\n",
    "        loss_vs_time.append(val_loss)\n",
    "        return \"Loglikelihood\", val_loss\n",
    "    \n",
    "    return log_loss, lambda: loss_vs_time\n",
    "\n",
    "log_loss, get_loss_vs_time = get_loss_logger()\n",
    "\n",
    "def timer():\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    def log_time(params, epoch):\n",
    "        nonlocal start\n",
    "        now = time.perf_counter()\n",
    "        delta = now - start\n",
    "        start = now\n",
    "        return \"Time\", delta\n",
    "\n",
    "    return log_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop with Adam optimizer\n",
    "\n",
    "Here we define a standard [JAX-experimental](https://github.com/google/jax/tree/master/jax/experimental) training loop using Adam with default parameters. We only add the `loggers` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_update(get_params, opt_update, gradients):\n",
    "\n",
    "    @jit\n",
    "    def update(i, opt_state, batch):\n",
    "        params = get_params(opt_state)\n",
    "        grads = gradients(params, batch)\n",
    "        return opt_update(i, grads, opt_state)\n",
    "\n",
    "    return update\n",
    "\n",
    "def train(params, x, gradients,\n",
    "          epochs = 100, batch_size = 10, lr = 1e-3, shuffle = True,\n",
    "          loggers = [], log_every = 10):\n",
    "\n",
    "    opt_init, opt_update, get_params = opt.adam(lr)\n",
    "    update = get_opt_update(get_params, opt_update, gradients)\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    for i in range(1, epochs + 1):\n",
    "        # TODO: shuffle in-place to reduce memory allocations (first, copy data)\n",
    "        x, _ = shuffle_perm(x) if shuffle else (x, None)\n",
    "        batches = batchify(x, batch_size)\n",
    "        for batch in batches:\n",
    "            opt_state = update(i, opt_state, batch)\n",
    "\n",
    "        if i % log_every == 0:\n",
    "            params = get_params(opt_state)\n",
    "            logs = [log(params, i) for log in loggers]\n",
    "            print(f\"Epoch {i}\", end=\" \")\n",
    "            for log in logs:\n",
    "                print('[%s %.2f]' % (log))\n",
    "            print()\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "See it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwise loss: Traced<ShapedArray(float32[]):JaxprTrace(level=-1/1)> Traced<ShapedArray(float32[]):JaxprTrace(level=-1/1)> Traced<ShapedArray(float32[]):JaxprTrace(level=-1/1)>\n",
      "Epoch 1 [Loglikelihood -3224.93]\n",
      "[Time 45.51]\n",
      "\n",
      "Epoch 2 [Loglikelihood -5264.71]\n",
      "[Time 44.57]\n",
      "\n",
      "Epoch 3 [Loglikelihood -3089.60]\n",
      "[Time 45.42]\n",
      "\n",
      "Epoch 4 [Loglikelihood -2941.20]\n",
      "[Time 47.39]\n",
      "\n",
      "Epoch 5 [Loglikelihood -2615.04]\n",
      "[Time 42.68]\n",
      "\n",
      "Epoch 6 [Loglikelihood -2390.87]\n",
      "[Time 44.15]\n",
      "\n",
      "Epoch 7 [Loglikelihood -2249.25]\n",
      "[Time 43.84]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(params, x, gradient,\n",
    "      epochs     = n_epochs,\n",
    "      lr         = lr, \n",
    "      batch_size = batch_size,\n",
    "      loggers    = [log_loss, timer()],\n",
    "      log_every  = 1\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('-log P(x)', fontsize='xx-large')\n",
    "ax.set_xlabel('Epochs', fontsize='xx-large')\n",
    "plt.plot(get_loss_vs_time())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
