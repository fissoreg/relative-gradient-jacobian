{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST training example\n",
    "\n",
    "This notebook presents a full commented implementation of the jacobian optimization method introduced in [1][Relative gradient optimization of the Jacobian term in unsupervised deep learning](https://arxiv.org/pdf/2006.15090.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The method has been implemented using Python and [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch               as t\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd      as ad\n",
    "import torch.optim         as optim\n",
    "\n",
    "import time\n",
    "\n",
    "from utils import shuffle_perm, batchify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers   = 2\n",
    "lr         = 1e-4\n",
    "n_epochs   = 10\n",
    "batch_size = 10\n",
    "\n",
    "log_every  = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training dataset\n",
    "\n",
    "We load the MNIST training and validation datasets as simple matrices of size NxD where N will usually be the `batch_size` and D = 784 (MNIST flattened dimensionality). As the first dimension is the batch dimension, we assume the samples to be arranged by rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('experiments/')\n",
    "from datasets.dataloaders import load_dataset\n",
    "x, val_data, _, _ = load_dataset(\"MNIST\")\n",
    "\n",
    "x = t.from_numpy(x).float()\n",
    "val_data = t.from_numpy(val_data).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "We implement the `smooth_leaky_relu` activation function\n",
    "$s_L(x) = \\alpha x + (1 - \\alpha) \\log (1 + e^x)$ and the `Dense` fully-connected network we use. The need for the intermediate activations and the `delta` terms is detailed in Section 4 of [[1]](https://arxiv.org/pdf/2006.15090.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_leaky_relu(x, alpha=0.3):\n",
    "    \"\"\"Smoothed version of the leaky relu activation function\n",
    "    Inspiration:\n",
    "    https://stats.stackexchange.com/questions/329776/approximating-leaky-relu-with-a-differentiable-function\n",
    "    \"\"\"\n",
    "    return alpha*x + (1 - alpha)*(t.logaddexp(x, t.zeros(x.shape)))\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self, D,\n",
    "                 n_layers=2,\n",
    "                 nonlinearity=smooth_leaky_relu,\n",
    "                 init=nn.init.orthogonal_):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.D = D\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        self.activations = []\n",
    "\n",
    "        for _ in range(n_layers):\n",
    "            layer = nn.Linear(D, D)\n",
    "            init(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "            self.linear_layers.append(layer)\n",
    "\n",
    "    # Forward pass through the network.\n",
    "    # Returns the latent configurations `z` (output)\n",
    "    # and saves the activations at intermediate layers `ys`.\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.activations = []\n",
    "        z = nn.Flatten()(x)\n",
    "\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            y = layer(z)\n",
    "            self.activations.append(y)\n",
    "            z = self.nonlinearity(y)\n",
    "\n",
    "        # last layer (no nonlinearity)\n",
    "        z = self.linear_layers[-1](z)\n",
    "\n",
    "        # grads of activations/output represent the 'delta' terms\n",
    "        # (the backpropagated error)\n",
    "        for y in self.activations:\n",
    "            y.retain_grad()\n",
    "        z.retain_grad()\n",
    "\n",
    "        return z\n",
    "\n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "    \n",
    "D = x.shape[-1]\n",
    "model = DenseNet(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The objective function to optimize is the loglikelihood of our model $\\boldsymbol{L}(\\boldsymbol{x}) = \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})$ that we rewrite as $\\boldsymbol{L}(\\boldsymbol{x}) = \\boldsymbol{L}_p(\\boldsymbol{x}) + \\boldsymbol{L}_J(\\boldsymbol{x})$ with\n",
    "\n",
    "$$\n",
    "\\boldsymbol{L}_p (\\boldsymbol{x}) = \\sum_i \\log p_i(\\boldsymbol{g}^i_{\\boldsymbol{\\theta}}(\\boldsymbol{x})) ; \\enspace \\ \\boldsymbol{L}_J (\\boldsymbol{x}) = \\log \\left|\\det \\boldsymbol{J}\\boldsymbol{g}_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\\right|\\,, \n",
    "$$\n",
    "\n",
    "We further separate $\\boldsymbol{L}_J$ in a \"layerwise\" component $\\boldsymbol{L}_J^1$ depending on the intermediate layers activations $\\boldsymbol{y}_k$ and a component $\\boldsymbol{L}_J^2$ depending on the parameters only\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " \\boldsymbol{L}_J (\\boldsymbol{z}_{k-1}) &= \\sum_{i=1}^D\\log \\left| \\sigma'(y_{k}^{i}) \\right| + \\log \\left| \\det \\boldsymbol{W}_k \\right| \\\\\n",
    "    &=: \\boldsymbol{L}_J^{1}(\\boldsymbol{y}_{k}) + \\boldsymbol{L}_J^{2}(\\boldsymbol{z}_{k-1})\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The gradient of the $\\boldsymbol{L}_J^2$ term is calculated explicitly and can be computed efficiently by making use of the relative gradient trick (see Section 4 of [[1]](https://arxiv.org/pdf/2006.15090.pdf)). Through automatic differentiation we optimize the latent variables distribution of choice $\\boldsymbol{L}_p$ (a standard Gaussian in this case) and the `loss_layerwise` term $\\boldsymbol{L}_J^1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pdf = t.distributions.MultivariateNormal(t.zeros(D), t.eye(D)).log_prob\n",
    "\n",
    "def loss_layerwise(f):\n",
    "    \n",
    "    def g(x):\n",
    "        y = f(x)\n",
    "\n",
    "        # we need to backprop through this computation,\n",
    "        # hence `create_graph=True`;\n",
    "        # also, `grad()` returns a tuple that we need to unpack\n",
    "        dfdx = ad.grad(y, x, t.ones_like(x), create_graph=True)[0]\n",
    "        log_dfdx = t.log(dfdx)\n",
    "\n",
    "        # summing over dimensions\n",
    "        return t.sum(log_dfdx, axis=-1)\n",
    "    \n",
    "    return g\n",
    "\n",
    "def loss(model, x):\n",
    "    z = model(x)\n",
    "    ys = t.stack(model.get_activations())\n",
    "\n",
    "    lpdf = log_pdf(z)\n",
    "    lwise = loss_layerwise(model.nonlinearity)(ys)\n",
    "\n",
    "    # note: we minimize the negative of the LL -> maximize the LL\n",
    "    return - t.mean(lpdf) - t.mean(lwise), z\n",
    "\n",
    "# Function to compute the term L^2_J of the loglikelihood\n",
    "def log_abs_det(model):\n",
    "    p = list(model.parameters())\n",
    "    ldet = sum([t.slogdet(W)[1] for W in p[::2]])\n",
    "\n",
    "    return ldet\n",
    "\n",
    "# Note that here we want to log the full loglikelihood;\n",
    "# during training we directly optimize only the term `lp + l_j^1`\n",
    "# and we include the gradient of the term `l_j^2` explicitly\n",
    "# (i.e. the `loss` function we derive includes only `lp + l_j^1`\n",
    "# and the `l_j^2` term is introduced with `add_det_grad`)\n",
    "def full_loss(model, x):\n",
    "    l1, z = loss(model, x)\n",
    "    l2 = log_abs_det(model)\n",
    "\n",
    "    return l2 - l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relative gradient\n",
    "\n",
    "Here we implement the functions to compute the relative gradient, applying the relative trick and taking care of avoiding matrix-matrix multiplications as described in Section 4 of [[1]](https://arxiv.org/pdf/2006.15090.pdf).\n",
    "\n",
    "The core of the computation happens in the `apply_trick` function, where the updates are computed following Appendix F of [[1]](https://arxiv.org/pdf/2006.15090.pdf) to include biases (note that we transposed the update expressions as we are working with row vectors).\n",
    "\n",
    "The `add_det_grad` function completes the gradient updates by including the gradients of the $\\boldsymbol{L}_J^2$ term of the loglikelihood, that we compute explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_trick(Wb, z, bp_terms):\n",
    "    W, b = Wb\n",
    "\n",
    "    # note that the nn.Linear layer in PyTorch performs the operation\n",
    "    # `y = xW^T + b`, so here we exchange the transposition to adhere\n",
    "    # to the notation in [1]\n",
    "    W = W.T\n",
    "\n",
    "    db, delta = bp_terms\n",
    "    dW = t.mm(t.mm(W, t.mm(W.T, z.T)), delta) + t.outer(t.mv(W, b), db)\n",
    "    db = db * (1 + t.dot(b, b.T)) + t.mv(delta.T, t.mv(z, t.mv(W, b)))\n",
    "\n",
    "    # back to \"PyTorch notation\"\n",
    "    dW = dW.T\n",
    "\n",
    "    return dW, db\n",
    "\n",
    "def add_det_grad(W):\n",
    "    return W.grad - W\n",
    "\n",
    "def compute_relative_gradient(model, x, z):\n",
    "    layers = model.linear_layers\n",
    "    ys = model.get_activations()\n",
    "\n",
    "    # collecting the backprop errors `deltas`\n",
    "    deltas = [y.grad for y in ys] + [z.grad]\n",
    "\n",
    "    # collecting intermediate activations + nonlinearity\n",
    "    zs = [x] + [model.nonlinearity(y) for y in ys]\n",
    "\n",
    "    for (z, delta, layer) in zip(zs, deltas, layers):\n",
    "\n",
    "        W, b = list(layer.parameters())\n",
    "        dW, db = W.grad, b.grad\n",
    "\n",
    "        dW, db = apply_trick((W, b), z, (db, delta))\n",
    "\n",
    "        # update gradients\n",
    "        W.grad = dW\n",
    "        b.grad = db\n",
    "\n",
    "        dW = add_det_grad(W)\n",
    "\n",
    "        # update gradients\n",
    "        W.grad = dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "Here we define a list of `loggers` to log scalar values. Each logger must be a function taking arguments `model` and `epoch` and must return a tuple: the first element is a string indicating the name of the logged quantity, the second element is the value to log. We define in this way a function `log_loss` to log the loglikelihood on the validation set during training, and a `log_time` function to log the execution time of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_logger():\n",
    "    loss_vs_time = []\n",
    "\n",
    "    def log_loss(model, epoch):\n",
    "        val_loss = full_loss(model, val_data)\n",
    "        loss_vs_time.append(val_loss)\n",
    "        return \"Loglikelihood\", val_loss\n",
    "\n",
    "    return log_loss, lambda: loss_vs_time\n",
    "\n",
    "log_loss, get_loss_vs_time = get_loss_logger()\n",
    "\n",
    "def timer():\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    def log_time(model, epoch):\n",
    "        nonlocal start\n",
    "        now = time.perf_counter()\n",
    "        delta = now - start\n",
    "        start = now\n",
    "        return \"Time\", delta\n",
    "\n",
    "    return log_time\n",
    "\n",
    "loggers = [log_loss, timer()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop with Adam optimizer\n",
    "\n",
    "Here we define a standard training loop using Adam with default parameters. After the backward pass we can use the accumulated gradients to compute the relative gradient of the parameters (using the `compute_relative_gradient` function previously defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Loglikelihood -1413.09] [Time 140.73] \n",
      "Epoch 2 [Loglikelihood -1389.43] [Time 95.65] \n",
      "Epoch 3 [Loglikelihood -1384.27] [Time 96.91] \n",
      "Epoch 4 [Loglikelihood -1382.18] [Time 96.52] \n",
      "Epoch 5 [Loglikelihood -1381.01] [Time 98.49] \n",
      "Epoch 6 [Loglikelihood -1380.47] [Time 99.45] \n",
      "Epoch 7 [Loglikelihood -1379.92] [Time 99.15] \n",
      "Epoch 8 [Loglikelihood -1379.70] [Time 99.28] \n",
      "Epoch 9 [Loglikelihood -1379.87] [Time 99.72] \n",
      "Epoch 10 [Loglikelihood -1380.94] [Time 99.37] \n",
      "Epoch 11 [Loglikelihood -1379.88] [Time 99.75] \n",
      "Epoch 12 [Loglikelihood -1380.39] [Time 99.28] \n",
      "Epoch 13 [Loglikelihood -1380.38] [Time 99.68] \n",
      "Epoch 14 [Loglikelihood -1380.10] [Time 99.97] \n",
      "Epoch 15 [Loglikelihood -1380.73] [Time 101.06] \n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for i in range(1, n_epochs + 1):\n",
    "\n",
    "    # TODO: shuffle in-place to reduce memory allocations (first, copy data)\n",
    "    x, _ = shuffle_perm(x)\n",
    "    batches = batchify(x, batch_size)\n",
    "\n",
    "    for batch in batches:\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        l, z = loss(model, batch)\n",
    "        l.backward()\n",
    "\n",
    "        compute_relative_gradient(model, batch, z)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    if i % log_every == 0:\n",
    "        logs = [log(model, i) for log in loggers]\n",
    "        print(f\"Epoch {i}\", end=\" \")\n",
    "        for log in logs:\n",
    "            print('[%s %.2f]' % (log), end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loglikelihood evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('log P(x)', fontsize='xx-large')\n",
    "ax.set_xlabel('Epochs', fontsize='xx-large')\n",
    "plt.plot(get_loss_vs_time())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
